#!/usr/bin/env npx tsx
/**
 * CLI Mogogo ‚Äî Joue des sessions compl√®tes sans passer par l'app mobile ni Supabase.
 *
 * Usage :
 *   npx tsx scripts/cli-session.ts --batch --context '{"social":"Amis","energy":4,"budget":"Standard","environment":"Ext√©rieur"}' --choices "A,B,A" --json
 *   npx tsx scripts/cli-session.ts --social "Amis" --energy 4 --budget "Standard" --env "Ext√©rieur"
 */

import { readFileSync, writeFileSync } from "node:fs";
import { createInterface } from "node:readline";
import { resolve } from "node:path";

// ---------------------------------------------------------------------------
// Charger .env.cli
// ---------------------------------------------------------------------------
try {
  const envPath = resolve(import.meta.dirname ?? ".", "../.env.cli");
  const envContent = readFileSync(envPath, "utf-8");
  for (const line of envContent.split("\n")) {
    const trimmed = line.trim();
    if (!trimmed || trimmed.startsWith("#")) continue;
    const eqIdx = trimmed.indexOf("=");
    if (eqIdx === -1) continue;
    const key = trimmed.slice(0, eqIdx).trim();
    const value = trimmed.slice(eqIdx + 1).trim();
    if (!process.env[key]) process.env[key] = value;
  }
} catch {
  // .env.cli absent ‚Üí on compte sur les env vars d√©j√† d√©finies
}

// ---------------------------------------------------------------------------
// Types (copi√©s depuis src/types/index.ts ‚Äî pas d'import alias @/ avec tsx)
// ---------------------------------------------------------------------------
type ActionType = "maps" | "web" | "steam" | "app_store" | "play_store" | "youtube" | "streaming" | "spotify";
interface Action { type: ActionType; label: string; query: string; }

interface LLMResponse {
  statut: "en_cours" | "finalis√©";
  phase: "questionnement" | "pivot" | "breakout" | "resultat";
  mogogo_message: string;
  question?: string;
  options?: { A: string; B: string };
  recommandation_finale?: {
    titre: string;
    explication: string;
    google_maps_query?: string;
    actions: Action[];
    tags?: string[];
  };
  metadata: { pivot_count: number; current_branch: string; depth?: number };
}

interface UserContext {
  social: string;
  energy: number;
  budget: string;
  environment: string;
  location?: { latitude: number; longitude: number };
  timing?: string;
  language?: string;
  children_ages?: { min: number; max: number };
}

type FunnelChoice = "A" | "B" | "neither" | "any" | "reroll" | "refine" | "finalize";

interface HistoryEntry {
  response: LLMResponse;
  choice?: FunnelChoice;
  choiceLabel?: string;
}

interface SessionStep {
  step: number;
  response: LLMResponse;
  choice?: FunnelChoice;
  latencyMs: number;
}

interface SessionResult {
  steps: SessionStep[];
  totalDurationMs: number;
  finalResponse?: LLMResponse;
  pivotCount: number;
}

// ---------------------------------------------------------------------------
// SYSTEM_PROMPT (copie exacte de supabase/functions/llm-gateway/index.ts)
// ---------------------------------------------------------------------------
const DEFAULT_SYSTEM_PROMPT = `Tu es Mogogo, hibou magicien bienveillant. R√©ponds TOUJOURS en JSON strict :
{"statut":"en_cours|finalis√©","phase":"questionnement|pivot|breakout|resultat","mogogo_message":"‚â§100 chars","question":"‚â§80 chars","options":{"A":"‚â§50 chars","B":"‚â§50 chars"},"recommandation_finale":{"titre":"Nom","explication":"2-3 phrases max","actions":[{"type":"maps|web|steam|app_store|play_store|youtube|streaming|spotify","label":"Texte","query":"‚â§60 chars"}],"tags":["slug"]},"metadata":{"pivot_count":0,"current_branch":"Cat > Sous-cat","depth":1}}

ANGLE Q1 (varier obligatoirement) :
- Seul/Couple ‚Üí Finalit√© : "Cr√©er (cuisine, DIY, dessin...)" vs "Consommer (film, jeu, spectacle...)"
- Amis ‚Üí Logistique : "Cocon (film, cuisine, jeu...)" vs "Aventure (sortie, balade, lieu in√©dit...)"
- Famille ‚Üí Vibe : "Calme (lecture, spa, balade zen...)" vs "D√©foulement (sport, escape game, karaok√©...)"
Pivot depth==1 : CHANGE d'angle. Depth>=2 : m√™me angle, sous-options diff√©rentes. Chaque option = 3-4 exemples concrets entre parenth√®ses.

ENVIRONNEMENT :
- "Int√©rieur" ‚â† maison. = lieu couvert. Mixer domicile + lieu public couvert (cin√©ma, caf√©, mus√©e, bowling, escape game). JAMAIS 2 options "√† la maison".
- "Ext√©rieur" = plein air. "Peu importe" = libre.

INSOLITE (obligatoire 1x/session) : g√©ocaching, bar √† jeux, atelier DIY, expo immersive, karaok√©, impro, murder party, astronomie, float tank, lancer de hache, VR, silent disco, food tour...

BRANCHE : metadata.current_branch = chemin hi√©rarchique complet, depth = niveau (1=racine). Choix A/B ‚Üí ajouter au chemin, depth++.

CONVERGENCE : 3-5 questions max. Chaque Q sous-divise TOUTES les sous-cat√©gories de l'option choisie. Options A/B courtes, contrast√©es, concr√®tes.

LONGUEURS (STRICT, jamais d√©passer) : mogogo_message ‚â§100 chars, question ‚â§80 chars, options A/B ‚â§50 chars chacune. Les exemples concrets vont dans la question, PAS dans les options. Options = libell√© court uniquement.

NEITHER (pivot, incr√©mente pivot_count) :
- depth>=2 : RESTE dans cat√©gorie parente, alternatives RADICALEMENT DIFF√âRENTES dans le m√™me th√®me.
- depth==1 : pivot lat√©ral complet, CHANGE d'angle.

REROLL : m√™me th√©matique/branche, activit√© DIFF√âRENTE. REFINE : au minimum 2 questions cibl√©es sur l'activit√© (dur√©e, ambiance, format...), puis finalis√© avec une recommandation affin√©e.
pivot_count>=3 ‚Üí breakout Top 3 (cat√©gories DIFF√âRENTES).

FINALIS√â : titre pr√©cis, 2-3 phrases, 1-3 actions pertinentes :
- Lieu ‚Üí "maps", Jeu PC ‚Üí "steam"+"youtube", Jeu mobile ‚Üí "app_store"+"play_store", Film/s√©rie ‚Üí "streaming"+"youtube", Musique ‚Üí "spotify", Cours ‚Üí "youtube"+"web", Autre ‚Üí "web"
Tags : 1-3 parmi [sport,culture,gastronomie,nature,detente,fete,creatif,jeux,musique,cinema,voyage,tech,social,insolite]

ENFANTS : si children_ages, adapter STRICTEMENT √† la tranche d'√¢ge.
TIMING : "now"/absent = imm√©diat. Date ISO = adapter √† saison/jour.

FIABILIT√â (CRITIQUE, pas d'acc√®s Internet) :
- Lieux locaux : JAMAIS de nom sp√©cifique sauf ic√¥nes nationales (Tour Eiffel) ou grandes cha√Ænes (Path√©, UGC). Recommande une CAT√âGORIE ("un restaurant de ramen"). Query maps g√©n√©rique ("bowling Nantes").
- √âv√©nements : JAMAIS de spectacle/expo sp√©cifique avec date. Recommande le TYPE + action "web" pour programmation.
- Contenu num√©rique : titres CONNUS et √âTABLIS uniquement.

FORMAT (CRITIQUE ‚Äî non-respect = erreur) :
- Ta r√©ponse DOIT √™tre un JSON COMPLET et VALIDE. Rien avant ni apr√®s.
- TOUJOURS fermer toutes les accolades et crochets. JAMAIS de JSON tronqu√©.
- mogogo_message : TOUJOURS pr√©sent, 1 phrase courte ‚â§ 100 chars, texte brut sans formatage.
- question : texte brut ‚â§ 80 chars, JAMAIS de **gras**, *italique* ou markdown.
- options A/B : texte brut court ‚â§ 50 chars, JAMAIS vides, JAMAIS de markdown.
- query d'action : ‚â§ 60 chars, JAMAIS de "site:" ou op√©rateurs de recherche. Mots-cl√©s simples uniquement.
- explication : ‚â§ 200 chars.`;

// ---------------------------------------------------------------------------
// Language instructions for non-French LLM responses
// ---------------------------------------------------------------------------
const LANGUAGE_INSTRUCTIONS: Record<string, string> = {
  en: "IMPORTANT: You MUST respond entirely in English. All fields (mogogo_message, question, options, recommandation_finale) must be in English. Keep the JSON keys in French as specified in the schema.",
  es: "IMPORTANT: You MUST respond entirely in Spanish. All fields (mogogo_message, question, options, recommandation_finale) must be in Spanish. Keep the JSON keys in French as specified in the schema.",
};

// Machine key ‚Üí human-readable descriptions per language
const CONTEXT_DESCRIPTIONS: Record<string, Record<string, Record<string, string>>> = {
  social: {
    solo:    { fr: "Seul", en: "Alone", es: "Solo/a" },
    friends: { fr: "Amis", en: "Friends", es: "Amigos" },
    couple:  { fr: "Couple", en: "Couple", es: "Pareja" },
    family:  { fr: "Famille", en: "Family", es: "Familia" },
  },
  budget: {
    free:     { fr: "Gratuit", en: "Free", es: "Gratis" },
    budget:   { fr: "√âconomique", en: "Budget", es: "Econ√≥mico" },
    standard: { fr: "Standard", en: "Standard", es: "Est√°ndar" },
    luxury:   { fr: "Luxe", en: "Luxury", es: "Lujo" },
  },
  environment: {
    indoor:  { fr: "Int√©rieur", en: "Indoor", es: "Interior" },
    outdoor: { fr: "Ext√©rieur", en: "Outdoor", es: "Exterior" },
    any_env: { fr: "Peu importe", en: "No preference", es: "Da igual" },
  },
};

function describeContext(context: UserContext, lang: string): Record<string, unknown> {
  const described = { ...context } as Record<string, unknown>;
  for (const field of ["social", "budget", "environment"] as const) {
    const key = context[field] as string;
    const mapping = CONTEXT_DESCRIPTIONS[field]?.[key];
    if (mapping) {
      described[field] = mapping[lang] ?? mapping.en ?? key;
    }
  }
  // Enrich children_ages with a human-readable description
  const ages = context.children_ages;
  if (ages && typeof ages.min === "number" && typeof ages.max === "number") {
    const templates: Record<string, string> = {
      fr: `Enfants de ${ages.min} √† ${ages.max} ans`,
      en: `Children aged ${ages.min} to ${ages.max}`,
      es: `Ni√±os de ${ages.min} a ${ages.max} a√±os`,
    };
    described.children_ages = templates[lang] ?? templates.en;
  }
  return described;
}

// ---------------------------------------------------------------------------
// Configuration LLM
// ---------------------------------------------------------------------------
const LLM_API_URL = process.env.LLM_API_URL ?? "http://localhost:11434/v1";
const LLM_MODEL = process.env.LLM_MODEL ?? "gpt-oss:120b-cloud";
const LLM_API_KEY = process.env.LLM_API_KEY ?? "";
const LLM_TEMPERATURE = parseFloat(process.env.LLM_TEMPERATURE ?? "0.7");
const LLM_TIMEOUT_MS = 60_000;

// ---------------------------------------------------------------------------
// Sanitisation ‚Äî strip markdown et nettoyer les textes
// ---------------------------------------------------------------------------
function stripMarkdown(text: string): string {
  return text
    .replace(/\*\*([^*]+)\*\*/g, "$1")  // **bold** ‚Üí bold
    .replace(/\*([^*]+)\*/g, "$1")       // *italic* ‚Üí italic
    .replace(/__([^_]+)__/g, "$1")       // __bold__ ‚Üí bold
    .replace(/_([^_]+)_/g, "$1")         // _italic_ ‚Üí italic
    .replace(/`([^`]+)`/g, "$1")         // `code` ‚Üí code
    .trim();
}

function truncate(t: string, maxLen: number): string {
  if (t.length <= maxLen) return t;
  const cut = t.lastIndexOf(" ", maxLen - 1);
  return (cut > maxLen * 0.4 ? t.slice(0, cut) : t.slice(0, maxLen - 1)) + "‚Ä¶";
}

function sanitizeResponse(d: Record<string, unknown>): void {
  // Strip markdown + tronquer mogogo_message
  if (typeof d.mogogo_message === "string") {
    d.mogogo_message = truncate(stripMarkdown(d.mogogo_message), 120);
  }
  // Strip markdown + tronquer question
  if (typeof d.question === "string") {
    d.question = truncate(stripMarkdown(d.question), 100);
  }
  // Strip markdown, tronquer et valider options non-vides
  if (d.options && typeof d.options === "object") {
    const opts = d.options as Record<string, unknown>;
    if (typeof opts.A === "string") opts.A = truncate(stripMarkdown(opts.A), 60);
    if (typeof opts.B === "string") opts.B = truncate(stripMarkdown(opts.B), 60);
    if (!opts.A || (typeof opts.A === "string" && opts.A.trim() === "")) {
      console.error("  ‚ö†Ô∏è  Option A vide d√©tect√©e, fallback");
      opts.A = "Option A";
    }
    if (!opts.B || (typeof opts.B === "string" && opts.B.trim() === "")) {
      console.error("  ‚ö†Ô∏è  Option B vide d√©tect√©e, fallback");
      opts.B = "Option B";
    }
  }
  // Strip markdown de recommandation_finale
  if (d.recommandation_finale && typeof d.recommandation_finale === "object") {
    const rec = d.recommandation_finale as Record<string, unknown>;
    if (typeof rec.titre === "string") rec.titre = stripMarkdown(rec.titre);
    if (typeof rec.explication === "string") rec.explication = stripMarkdown(rec.explication);
  }
}

// ---------------------------------------------------------------------------
// Validation (synchronis√©e avec src/services/llm.ts)
// ---------------------------------------------------------------------------
function validateLLMResponse(data: unknown): LLMResponse {
  if (!data || typeof data !== "object") {
    throw new Error("R√©ponse LLM invalide : objet attendu");
  }
  const d = data as Record<string, unknown>;

  // Le LLM copie parfois le format compress√© de l'historique (q/A/B au lieu du format complet)
  // Tenter de reconstruire une r√©ponse valide
  if (!d.statut && d.q && (d.A || d.B)) {
    d.statut = "en_cours";
    d.phase = d.phase ?? "questionnement";
    d.question = d.q as string;
    d.options = { A: (d.A as string) ?? "", B: (d.B as string) ?? "" };
    if (!d.mogogo_message) d.mogogo_message = "Hmm, voyons...";
    if (!d.metadata) {
      d.metadata = {
        pivot_count: 0,
        current_branch: (d.branch as string) ?? "Racine",
        depth: (d.depth as number) ?? 1,
      };
    }
    console.error("  ‚ö†Ô∏è  R√©ponse compress√©e d√©tect√©e, reconstruction");
  }

  if (!["en_cours", "finalis√©"].includes(d.statut as string)) {
    throw new Error("R√©ponse LLM invalide : statut manquant ou incorrect");
  }
  if (
    !["questionnement", "pivot", "breakout", "resultat"].includes(
      d.phase as string,
    )
  ) {
    throw new Error("R√©ponse LLM invalide : phase manquante ou incorrecte");
  }
  // R√©cup√©rer mogogo_message si manquant
  if (typeof d.mogogo_message !== "string" || !d.mogogo_message.trim()) {
    // Tenter de r√©cup√©rer depuis d'autres champs ou fournir un fallback
    if (typeof d.message === "string" && d.message.trim()) {
      d.mogogo_message = d.message;
      console.error("  ‚ö†Ô∏è  mogogo_message absent, r√©cup√©r√© depuis 'message'");
    } else if (typeof d.question === "string" && d.question.trim()) {
      d.mogogo_message = "Hmm, laisse-moi r√©fl√©chir...";
      console.error("  ‚ö†Ô∏è  mogogo_message absent, fallback utilis√©");
    } else {
      throw new Error("R√©ponse LLM invalide : mogogo_message manquant");
    }
  }

  // Sanitiser les textes (strip markdown, valider options non-vides)
  sanitizeResponse(d);

  // Normaliser les breakouts : le LLM renvoie parfois statut "en_cours" avec
  // un champ "breakout"/"breakout_options" au lieu de "finalis√©" + "recommandation_finale"
  if (d.phase === "breakout" && !d.recommandation_finale) {
    const breakoutArray = (d as any).breakout ?? (d as any).breakout_options;
    if (Array.isArray(breakoutArray) && breakoutArray.length > 0) {
      const items = breakoutArray as Array<{
        titre?: string; explication?: string; actions?: unknown[];
      }>;
      d.statut = "finalis√©";
      d.recommandation_finale = {
        titre: items.map(b => b.titre ?? "").filter(Boolean).join(" / "),
        explication: items.map(b => b.explication ?? "").filter(Boolean).join(" "),
        actions: items.flatMap(b => Array.isArray(b.actions) ? b.actions : []),
        tags: [],
      };
    }
  }

  // Le LLM met parfois statut "en_cours" sur un breakout qui a d√©j√† une recommandation_finale
  if (d.phase === "breakout" && d.statut === "en_cours" && d.recommandation_finale) {
    d.statut = "finalis√©";
  }

  // Si en_cours sans question mais avec recommandation_finale ‚Üí flip vers finalis√©
  if (d.statut === "en_cours" && !d.question && d.recommandation_finale) {
    d.statut = "finalis√©";
    d.phase = "resultat";
    console.error("  ‚ö†Ô∏è  en_cours sans question mais avec reco ‚Üí flip vers finalis√©");
  }
  if (d.statut === "en_cours" && !d.question) {
    throw new Error(
      "R√©ponse LLM invalide : question manquante en phase en_cours",
    );
  }
  // Fallback options si manquantes en en_cours (JSON tronqu√© avant les options)
  if (d.statut === "en_cours" && d.question && (!d.options || typeof d.options !== "object")) {
    d.options = { A: "Option A", B: "Option B" };
    console.error("  ‚ö†Ô∏è  Options manquantes, fallback utilis√©");
  }
  if (d.statut === "finalis√©" && !d.recommandation_finale) {
    throw new Error(
      "R√©ponse LLM invalide : recommandation_finale manquante en phase finalis√©",
    );
  }
  // Normaliser : garantir que actions et tags existent toujours dans recommandation_finale
  if (d.recommandation_finale && typeof d.recommandation_finale === "object") {
    const rec = d.recommandation_finale as Record<string, unknown>;
    if (!Array.isArray(rec.actions)) {
      rec.actions = [];
      if (rec.google_maps_query && typeof rec.google_maps_query === "string") {
        rec.actions = [{ type: "maps", label: "Voir sur Maps", query: rec.google_maps_query }];
      }
    }
    // Fallback : si titre pr√©sent mais actions vides, ajouter une action web g√©n√©rique
    if (Array.isArray(rec.actions) && rec.actions.length === 0 && typeof rec.titre === "string" && rec.titre.trim()) {
      rec.actions = [{ type: "web", label: "Rechercher", query: rec.titre }];
      console.error("  ‚ö†Ô∏è  Actions vides, fallback web ajout√©");
    }
    // Fallback : si explication manquante
    if (!rec.explication || (typeof rec.explication === "string" && !rec.explication.trim())) {
      rec.explication = rec.titre ?? "Activit√© recommand√©e par Mogogo";
      console.error("  ‚ö†Ô∏è  Explication manquante, fallback utilis√©");
    }
    if (!Array.isArray(rec.tags)) {
      rec.tags = [];
    } else {
      rec.tags = (rec.tags as unknown[]).filter((t: unknown) => typeof t === "string");
    }
  }
  // Garantir metadata
  if (!d.metadata || typeof d.metadata !== "object") {
    d.metadata = { pivot_count: 0, current_branch: "Racine", depth: 1 };
  }
  return data as LLMResponse;
}

// ---------------------------------------------------------------------------
// Appel LLM
// ---------------------------------------------------------------------------
async function callLLM(
  context: UserContext,
  history: HistoryEntry[],
  choice?: FunnelChoice,
  systemPrompt?: string,
): Promise<{ response: LLMResponse; latencyMs: number }> {
  const messages: Array<{ role: string; content: string }> = [
    { role: "system", content: systemPrompt ?? DEFAULT_SYSTEM_PROMPT },
  ];

  // Inject language instruction for non-French languages
  const lang = context.language ?? "fr";
  if (LANGUAGE_INSTRUCTIONS[lang]) {
    messages.push({ role: "system", content: LANGUAGE_INSTRUCTIONS[lang] });
  }

  // Translate machine keys to human-readable descriptions for the LLM
  const describedContext = describeContext(context, lang);
  messages.push({
    role: "user",
    content: `Contexte utilisateur : ${JSON.stringify(describedContext)}`,
  });

  // Enrichissement temporel pour les dates pr√©cises
  if (context.timing && context.timing !== "now") {
    const date = new Date(context.timing + "T12:00:00");
    if (!isNaN(date.getTime())) {
      const lang = context.language ?? "fr";
      const localeMap: Record<string, string> = { fr: "fr-FR", en: "en-US", es: "es-ES" };
      const locale = localeMap[lang] ?? "en-US";
      const dayName = date.toLocaleDateString(locale, { weekday: "long" });
      const dayNum = date.getDate();
      const month = date.toLocaleDateString(locale, { month: "long" });
      const year = date.getFullYear();
      const m = date.getMonth();
      const seasonNames: Record<string, Record<string, string>> = {
        fr: { spring: "printemps", summer: "√©t√©", autumn: "automne", winter: "hiver" },
        en: { spring: "spring", summer: "summer", autumn: "autumn", winter: "winter" },
        es: { spring: "primavera", summer: "verano", autumn: "oto√±o", winter: "invierno" },
      };
      const seasonKey = m >= 2 && m <= 4 ? "spring" : m >= 5 && m <= 7 ? "summer" : m >= 8 && m <= 10 ? "autumn" : "winter";
      const season = seasonNames[lang]?.[seasonKey] ?? seasonNames.en[seasonKey];
      const templates: Record<string, string> = {
        fr: `Info temporelle : l'activit√© est pr√©vue pour le ${dayName} ${dayNum} ${month} ${year} (saison : ${season}).`,
        en: `Temporal info: the activity is planned for ${dayName} ${dayNum} ${month} ${year} (season: ${season}).`,
        es: `Info temporal: la actividad est√° prevista para el ${dayName} ${dayNum} de ${month} de ${year} (temporada: ${season}).`,
      };
      messages.push({
        role: "user",
        content: templates[lang] ?? templates.en,
      });
    }
  }

  // Helper: compute depth (consecutive A/B choices) at a given position in history
  function computeDepthAt(hist: HistoryEntry[], endIdx: number): { depth: number; chosenPath: string[] } {
    let depth = 1;
    const chosenPath: string[] = [];
    for (let i = endIdx; i >= 0; i--) {
      const c = hist[i]?.choice;
      if (c === "A" || c === "B") {
        depth++;
        const opts = hist[i]?.response?.options;
        if (opts && opts[c]) {
          chosenPath.unshift(opts[c]);
        }
      } else {
        break;
      }
    }
    return { depth, chosenPath };
  }

  // Historique compress√© (comme l'Edge Function) pour √©conomiser des tokens
  for (let idx = 0; idx < history.length; idx++) {
    const entry = history[idx];
    const r = entry.response;
    const compressed: Record<string, unknown> = {
      q: r.question,
      A: r.options?.A,
      B: r.options?.B,
      phase: r.phase,
    };
    if (r.metadata?.current_branch) compressed.branch = r.metadata.current_branch;
    if (r.metadata?.depth) compressed.depth = r.metadata.depth;
    messages.push({ role: "assistant", content: JSON.stringify(compressed) });
    if (entry.choice) {
      messages.push({ role: "user", content: `Choix : ${entry.choice}` });
    }
  }

  // Post-refine enforcement: si un "refine" a √©t√© fait r√©cemment dans l'historique
  // et que moins de 2 questions ont √©t√© pos√©es depuis, forcer le LLM √† continuer
  if (history.length > 0 && choice && choice !== "refine") {
    let refineIdx = -1;
    for (let i = history.length - 1; i >= 0; i--) {
      if (history[i].choice === "refine") { refineIdx = i; break; }
    }
    if (refineIdx >= 0) {
      const questionsSinceRefine = history.length - 1 - refineIdx;
      if (questionsSinceRefine < 2) {
        const remaining = 2 - questionsSinceRefine;
        messages.push({ role: "system", content: `DIRECTIVE SYST√àME : Affinage en cours (${questionsSinceRefine}/2 questions pos√©es). Tu DOIS poser encore au minimum ${remaining} question(s) cibl√©e(s) sur l'activit√© (dur√©e, ambiance, format, lieu...) avant de finaliser. R√©ponds OBLIGATOIREMENT avec statut "en_cours" et phase "questionnement".` });
      }
    }
  }

  if (choice) {
    if (choice === "neither" && history.length > 0) {
      const { depth, chosenPath } = computeDepthAt(history, history.length - 1);
      if (depth >= 2) {
        const parentTheme = chosenPath[chosenPath.length - 1] ?? chosenPath[0] ?? "ce th√®me";
        messages.push({
          role: "system",
          content: `DIRECTIVE SYST√àME : L'utilisateur a rejet√© ces deux sous-options PR√âCISES, mais il aime toujours la cat√©gorie parente "${parentTheme}". Tu DOIS rester dans ce th√®me et proposer deux alternatives RADICALEMENT DIFF√âRENTES au sein de "${parentTheme}". NE CHANGE PAS de cat√©gorie. Profondeur = ${depth}, chemin = "${chosenPath.join(" > ")}".`,
        });
      } else {
        messages.push({
          role: "system",
          content: `DIRECTIVE SYST√àME : Pivot complet. L'utilisateur rejette d√®s la racine. Change totalement d'angle d'attaque.`,
        });
      }
      messages.push({ role: "user", content: `Choix : neither` });
    } else if (choice === "finalize") {
      messages.push({
        role: "system",
        content: `DIRECTIVE SYST√àME : L'utilisateur veut un r√©sultat MAINTENANT. Tu DOIS r√©pondre avec statut "finalis√©", phase "resultat" et une recommandation_finale concr√®te bas√©e sur les choix d√©j√† faits dans l'historique. Ne pose AUCUNE question suppl√©mentaire.`,
      });
      messages.push({ role: "user", content: `Choix : finalize` });
    } else if (choice === "refine") {
      messages.push({
        role: "system",
        content: `DIRECTIVE SYST√àME : L'utilisateur veut AFFINER sa recommandation. Tu DOIS poser au minimum 2 questions cibl√©es sur l'activit√© recommand√©e (dur√©e, ambiance, format, lieu pr√©cis...) AVANT de finaliser. R√©ponds avec statut "en_cours", phase "questionnement". NE finalise PAS maintenant.`,
      });
      messages.push({ role: "user", content: `Choix : refine` });
    } else {
      messages.push({ role: "user", content: `Choix : ${choice}` });
    }
  }

  // max_tokens adaptatif : steps interm√©diaires = concis, finalize/breakout = plus de place
  const isFinalStep = choice === "finalize" || choice === "reroll";
  const maxTokens = isFinalStep ? 3000 : 2000;

  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), LLM_TIMEOUT_MS);

  const start = Date.now();
  try {
    const res = await fetch(`${LLM_API_URL}/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(LLM_API_KEY ? { Authorization: `Bearer ${LLM_API_KEY}` } : {}),
      },
      body: JSON.stringify({
        model: LLM_MODEL,
        messages,
        temperature: LLM_TEMPERATURE,
        max_tokens: maxTokens,
        response_format: { type: "json_object" },
      }),
      signal: controller.signal,
    });

    if (!res.ok) {
      const errorText = await res.text();
      throw new Error(`LLM API ${res.status}: ${errorText.slice(0, 200)}`);
    }

    const data = await res.json();
    const content = data.choices?.[0]?.message?.content;
    if (!content) {
      throw new Error("R√©ponse LLM vide");
    }

    const parsed = JSON.parse(content);
    const response = validateLLMResponse(parsed);
    return { response, latencyMs: Date.now() - start };
  } finally {
    clearTimeout(timeoutId);
  }
}

// ---------------------------------------------------------------------------
// Affichage
// ---------------------------------------------------------------------------
function printBreadcrumb(history: HistoryEntry[]) {
  const labels = history
    .filter((h): h is HistoryEntry & { choice: "A" | "B" } => h.choice === "A" || h.choice === "B")
    .map(h => h.response.options?.[h.choice] ?? h.choice);
  if (labels.length > 0) {
    console.error(`  üìç ${labels.join(" > ")}`);
  }
}

function printStep(
  step: number,
  response: LLMResponse,
  latencyMs: number,
  choice?: FunnelChoice,
  jsonMode = false,
) {
  if (jsonMode) {
    const obj: SessionStep = { step, response, latencyMs, ...(choice !== undefined ? { choice } : {}) };
    process.stdout.write(JSON.stringify(obj) + "\n");
    return;
  }

  const phaseTag = `[${response.phase}]`.padEnd(18);
  console.error(`\n‚îÄ‚îÄ Step ${step} ${phaseTag} (${latencyMs}ms) ‚îÄ‚îÄ`);
  console.error(`ü¶â ${response.mogogo_message}`);

  if (response.statut === "en_cours" && response.question) {
    console.error(`\n‚ùì ${response.question}`);
    if (response.options) {
      console.error(`   A) ${response.options.A}`);
      console.error(`   B) ${response.options.B}`);
    }
  }

  if (response.recommandation_finale?.titre) {
    const rec = response.recommandation_finale;
    console.error(`\nüéØ ${rec.titre}`);
    console.error(`   ${rec.explication}`);
    const ACTION_ICONS: Record<string, string> = {
      maps: "üìç", steam: "üéÆ", web: "üåê", youtube: "‚ñ∂Ô∏è",
      app_store: "üçé", play_store: "üì±", streaming: "üé¨", spotify: "üéµ",
    };
    if (rec.actions?.length) {
      for (const a of rec.actions) {
        console.error(`   ${ACTION_ICONS[a.type] ?? "üîó"} [${a.type}] ${a.label} ‚Üí ${a.query}`);
      }
    }
    if (rec.tags?.length) {
      console.error(`   üè∑Ô∏è  ${rec.tags.join(", ")}`);
    }
  }

  if (response.metadata) {
    console.error(
      `   pivot_count=${response.metadata.pivot_count}  branch=${response.metadata.current_branch}`,
    );
  }

  if (choice !== undefined) {
    console.error(`   ‚Üí Choix : ${choice}`);
  }
}

// ---------------------------------------------------------------------------
// Providers de choix
// ---------------------------------------------------------------------------
type ChoiceProvider = (currentResponse: LLMResponse) => Promise<FunnelChoice>;

function batchProvider(choices: FunnelChoice[]): ChoiceProvider {
  let idx = 0;
  return async (_response: LLMResponse) => {
    if (idx < choices.length) return choices[idx++];
    return "A"; // d√©faut si √©puis√©
  };
}

function interactiveProvider(): ChoiceProvider {
  const rl = createInterface({ input: process.stdin, output: process.stderr });
  return (response: LLMResponse) =>
    new Promise<FunnelChoice>((resolve) => {
      const prompt = response.statut === "finalis√©"
        ? "\nReroll ? (reroll / n) : "
        : "\nChoix (A / B / neither / any / /back [N]) : ";
      rl.question(
        prompt,
        (answer: string) => {
          const cleaned = answer.trim();
          const backMatch = cleaned.match(/^\/back\s*(\d*)$/i);
          if (backMatch) {
            const idx = backMatch[1] ? parseInt(backMatch[1], 10) : undefined;
            resolve(`__back:${idx ?? "last"}` as FunnelChoice);
            return;
          }
          const lower = cleaned.toLowerCase();
          if (["a", "b", "neither", "any", "reroll"].includes(lower)) {
            resolve(lower === "a" ? "A" : lower === "b" ? "B" : (lower as FunnelChoice));
          } else if (response.statut === "finalis√©") {
            resolve("A"); // pas de reroll ‚Üí fin
          } else {
            console.error(`  Choix invalide "${answer}", d√©faut ‚Üí A`);
            resolve("A");
          }
        },
      );
    });
}

function parseAutoChoice(raw: string): FunnelChoice {
  const result = parseAutoChoiceInner(raw);
  console.error(`   [auto] ‚Üí ${result}`);
  return result;
}

function parseAutoChoiceInner(raw: string): FunnelChoice {
  if (!raw) return "A";

  // 1. Priorit√© absolue : derni√®re ligne non-vide (format attendu du prompt)
  const lines = raw.split("\n").map((l: string) => l.trim()).filter((l: string) => l.length > 0);
  const lastLine = (lines.at(-1) ?? "").toUpperCase().replace(/[^A-Z]/g, "");
  if (lastLine === "A" || lastLine === "B") return lastLine as FunnelChoice;
  if (lastLine === "NEITHER" || lastLine === "AUCUNE" || lastLine === "AUCUNEDESDEUX") return "neither";
  if (lastLine === "ANY") return "any";

  // 2. Derni√®res lignes courtes (‚â§ 30 chars) ‚Äî le LLM met parfois un r√©sum√© avant la lettre
  for (let i = lines.length - 1; i >= Math.max(0, lines.length - 3); i--) {
    const short = lines[i].toUpperCase().replace(/[^A-Z]/g, "");
    if (short === "A" || short === "B") return short as FunnelChoice;
    if (lines[i].length <= 30) {
      if (short.includes("NEITHER") || short.includes("AUCUNE")) return "neither";
      if (short.includes("ANY") || short.includes("INDIFFEREN")) return "any";
    }
  }

  // 3. Pattern explicite dans tout le texte ("choix B", "option A", "r√©ponse : B")
  const upper = raw.toUpperCase();
  const explicitMatches = [...upper.matchAll(/(?:CHOIX|OPTION|R√âPONSE|ANSWER|FINAL|CONCLUS)[^A-Z]{0,5}([AB])\b/g)];
  if (explicitMatches.length > 0) return explicitMatches.at(-1)![1] as FunnelChoice;

  // 4. Dernier recours : derni√®re lettre A ou B isol√©e
  const isolated = [...upper.matchAll(/(?:^|[^A-Z])([AB])(?:[^A-Z]|$)/g)];
  if (isolated.length > 0) return isolated.at(-1)![1] as FunnelChoice;

  return "A";
}

function autoProvider(persona: string): ChoiceProvider {
  return async (currentResponse: LLMResponse) => {
    const prompt = `Tu es un utilisateur avec cette intention pr√©cise : "${persona}"

On te propose cette question :
"${currentResponse.question}"
  A) ${currentResponse.options?.A}
  B) ${currentResponse.options?.B}

Analyse chaque option par rapport √† ton intention, puis donne ta r√©ponse finale.
Format strict ‚Äî derni√®re ligne = uniquement la lettre choisie : A ou B (ou neither si aucune ne correspond, any si les deux conviennent).`;

    const res = await fetch(`${LLM_API_URL}/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(LLM_API_KEY ? { Authorization: `Bearer ${LLM_API_KEY}` } : {}),
      },
      body: JSON.stringify({
        model: LLM_MODEL,
        messages: [{ role: "user", content: prompt }],
        temperature: 0.3,
        max_tokens: 500,
      }),
    });

    if (!res.ok) {
      console.error(`‚ö†Ô∏è  Auto-provider LLM error ${res.status}, fallback ‚Üí A`);
      return "A";
    }

    const data = await res.json();
    const msg = data.choices?.[0]?.message;
    // Le contenu peut √™tre dans content (mod√®le classique) ou reasoning (mod√®le raisonnement)
    const content = (msg?.content ?? "").trim();
    const reasoning = (msg?.reasoning ?? "").trim();
    const raw = content || reasoning;

    return parseAutoChoice(raw);
  };
}

// ---------------------------------------------------------------------------
// Session
// ---------------------------------------------------------------------------
async function runSession(
  context: UserContext,
  choiceProvider: ChoiceProvider,
  options: { systemPrompt?: string; maxSteps?: number; jsonMode?: boolean } = {},
): Promise<SessionResult> {
  const maxSteps = options.maxSteps ?? 20;
  const history: HistoryEntry[] = [];
  const steps: SessionStep[] = [];
  const sessionStart = Date.now();

  // Appel initial (pas de choix)
  const initial = await callLLM(context, history, undefined, options.systemPrompt);
  steps.push({ step: 1, response: initial.response, latencyMs: initial.latencyMs });
  printStep(1, initial.response, initial.latencyMs, undefined, options.jsonMode);

  if (initial.response.statut === "finalis√©") {
    return {
      steps,
      totalDurationMs: Date.now() - sessionStart,
      finalResponse: initial.response,
      pivotCount: initial.response.metadata?.pivot_count ?? 0,
    };
  }

  let currentResponse = initial.response;

  for (let i = 2; i <= maxSteps; i++) {
    const choice = await choiceProvider(currentResponse);

    // Time travel via /back [N]
    if (typeof choice === "string" && choice.startsWith("__back:")) {
      const rawIdx = choice.replace("__back:", "");
      const targetIdx = rawIdx === "last"
        ? history.length - 1
        : parseInt(rawIdx, 10);

      if (targetIdx < 0 || targetIdx >= history.length) {
        console.error(`  Index invalide: ${targetIdx} (0-${history.length - 1})`);
        i--;
        continue;
      }

      const truncated = history.slice(0, targetIdx);
      const target = history[targetIdx].response;
      const llmHistory: HistoryEntry[] = [...truncated, { response: target, choice: "neither" as FunnelChoice }];

      history.length = 0;
      history.push(...truncated);

      console.error(`\n  [time-travel] Retour au step ${targetIdx}`);
      printBreadcrumb(history);

      const result = await callLLM(context, llmHistory, "neither", options.systemPrompt);
      history.push({ response: target, choice: "neither" as FunnelChoice });
      currentResponse = result.response;

      steps.push({ step: i, response: result.response, choice: "neither" as FunnelChoice, latencyMs: result.latencyMs });
      printStep(i, result.response, result.latencyMs, "neither" as FunnelChoice, options.jsonMode);
      printBreadcrumb(history);
      continue;
    }

    history.push({ response: currentResponse, choice });

    const result = await callLLM(context, history, undefined, options.systemPrompt);
    steps.push({ step: i, response: result.response, choice, latencyMs: result.latencyMs });
    printStep(i, result.response, result.latencyMs, choice, options.jsonMode);
    printBreadcrumb(history);

    currentResponse = result.response;

    if (currentResponse.statut === "finalis√©") {
      // En batch/interactif : v√©rifier si le prochain choix est "reroll"
      const nextChoice = await choiceProvider(currentResponse);
      if (nextChoice === "reroll") {
        // Continuer la boucle ‚Äî le reroll sera trait√© au prochain tour
        history.push({ response: currentResponse, choice: nextChoice });
        const rerollResult = await callLLM(context, history, undefined, options.systemPrompt);
        i++;
        steps.push({ step: i, response: rerollResult.response, choice: nextChoice, latencyMs: rerollResult.latencyMs });
        printStep(i, rerollResult.response, rerollResult.latencyMs, nextChoice, options.jsonMode);
        currentResponse = rerollResult.response;
        continue;
      }
      return {
        steps,
        totalDurationMs: Date.now() - sessionStart,
        finalResponse: currentResponse,
        pivotCount: currentResponse.metadata?.pivot_count ?? 0,
      };
    }
  }

  console.error(`\n‚ö†Ô∏è  Max steps (${maxSteps}) atteint sans finalisation.`);
  return {
    steps,
    totalDurationMs: Date.now() - sessionStart,
    pivotCount: currentResponse.metadata?.pivot_count ?? 0,
  };
}

// ---------------------------------------------------------------------------
// Parsing CLI
// ---------------------------------------------------------------------------
function parseArgs(argv: string[]) {
  const args = argv.slice(2);
  const opts: Record<string, string | boolean> = {};
  for (let i = 0; i < args.length; i++) {
    const arg = args[i];
    if (arg === "--batch") {
      opts.batch = true;
    } else if (arg === "--auto") {
      opts.auto = true;
    } else if (arg === "--json") {
      opts.json = true;
    } else if (
      ["--context", "--choices", "--prompt-file", "--transcript", "--max-steps",
        "--social", "--energy", "--budget", "--env", "--persona", "--timing", "--lang",
        "--children-ages"].includes(arg)
    ) {
      opts[arg.replace(/^--/, "")] = args[++i] ?? "";
    } else if (arg === "--help" || arg === "-h") {
      opts.help = true;
    }
  }
  return opts;
}

function printUsage() {
  console.error(`Usage: npx tsx scripts/cli-session.ts [options]

Options:
  --batch                  Mode non-interactif
  --auto                   Mode auto (un LLM joue le r√¥le de l'utilisateur)
  --persona "..."          Intention de l'utilisateur simul√© (mode auto)
  --json                   Sortie JSON (une ligne par step sur stdout)
  --context '{...}'        Contexte utilisateur en JSON
  --social, --energy, --budget, --env   Contexte par champs s√©par√©s
  --children-ages "min,max"    Tranche d'√¢ge enfants (ex: "3,10") ‚Äî implique social=family
  --timing "now"|"YYYY-MM-DD"  Quand faire l'activit√© (d√©faut: now)
  --lang fr|en|es          Langue des r√©ponses LLM (d√©faut: fr)
  --choices "A,B,..."      Choix pr√©d√©finis (mode batch)
  --prompt-file <path>     System prompt alternatif depuis un fichier
  --transcript <path>      Sauvegarder la session compl√®te en JSON
  --max-steps N            Limite de steps (d√©faut 20)
  -h, --help               Afficher cette aide`);
}

// ---------------------------------------------------------------------------
// Main
// ---------------------------------------------------------------------------
async function main() {
  const opts = parseArgs(process.argv);

  if (opts.help) {
    printUsage();
    process.exit(0);
  }

  // Construire le contexte
  let context: UserContext;
  if (opts.context) {
    try {
      context = JSON.parse(opts.context as string);
    } catch {
      console.error("Erreur : --context doit √™tre un JSON valide.");
      process.exit(1);
    }
  } else if (opts.social || opts.energy || opts.budget || opts.env) {
    context = {
      social: (opts.social as string) ?? "Seul",
      energy: parseInt((opts.energy as string) ?? "3", 10),
      budget: (opts.budget as string) ?? "Standard",
      environment: (opts.env as string) ?? "Peu importe",
    };
  } else {
    console.error("Erreur : contexte requis. Utilisez --context ou --social/--energy/--budget/--env.");
    printUsage();
    process.exit(1);
  }

  // Ajouter timing si sp√©cifi√© via --timing (fonctionne avec --context et champs s√©par√©s)
  if (opts.timing) {
    context.timing = opts.timing as string;
  }

  // Ajouter la langue si sp√©cifi√©e via --lang
  if (opts.lang) {
    context.language = opts.lang as string;
  }

  // Ajouter children_ages si sp√©cifi√© via --children-ages "min,max"
  if (opts["children-ages"]) {
    const parts = (opts["children-ages"] as string).split(",").map(Number);
    if (parts.length === 2 && !isNaN(parts[0]) && !isNaN(parts[1])) {
      context.children_ages = { min: parts[0], max: parts[1] };
    } else {
      console.error("Erreur : --children-ages doit √™tre au format \"min,max\" (ex: \"3,10\").");
      process.exit(1);
    }
  }

  // System prompt
  let systemPrompt: string | undefined;
  if (opts["prompt-file"]) {
    try {
      systemPrompt = readFileSync(opts["prompt-file"] as string, "utf-8");
    } catch (e: any) {
      console.error(`Erreur lecture prompt-file : ${e.message}`);
      process.exit(1);
    }
  }

  // Choix provider
  const jsonMode = Boolean(opts.json);
  let choiceProvider: ChoiceProvider;
  if (opts.auto) {
    const persona = (opts.persona as string) ?? "Je cherche une activit√© sympa";
    choiceProvider = autoProvider(persona);
  } else if (opts.batch) {
    const choices = opts.choices
      ? (opts.choices as string).split(",").map((c) => c.trim() as FunnelChoice)
      : [];
    choiceProvider = batchProvider(choices);
  } else {
    choiceProvider = interactiveProvider();
  }

  const maxSteps = opts["max-steps"] ? parseInt(opts["max-steps"] as string, 10) : 20;

  if (!jsonMode) {
    const mode = opts.auto ? "auto" : opts.batch ? "batch" : "interactif";
    console.error(`\nü¶â Mogogo CLI ‚Äî ${mode}`);
    console.error(`   Model: ${LLM_MODEL} @ ${LLM_API_URL}`);
    console.error(`   Contexte: ${JSON.stringify(context)}`);
    if (opts.lang) console.error(`   Langue: ${opts.lang}`);
    if (opts.auto) console.error(`   Persona: ${opts.persona ?? "Je cherche une activit√© sympa"}`);
    if (systemPrompt) console.error(`   Prompt file: ${opts["prompt-file"]}`);
    console.error("");
  }

  try {
    const result = await runSession(context, choiceProvider, {
      systemPrompt,
      maxSteps,
      jsonMode,
    });

    if (!jsonMode) {
      console.error(`\n‚îÄ‚îÄ Session termin√©e ‚îÄ‚îÄ`);
      console.error(`   Steps: ${result.steps.length}`);
      console.error(`   Dur√©e totale: ${result.totalDurationMs}ms`);
      console.error(`   Pivot count: ${result.pivotCount}`);
      if (result.finalResponse?.recommandation_finale) {
        console.error(
          `   R√©sultat: ${result.finalResponse.recommandation_finale.titre}`,
        );
      }
    }

    // Transcript
    if (opts.transcript) {
      writeFileSync(
        opts.transcript as string,
        JSON.stringify(result, null, 2),
        "utf-8",
      );
      if (!jsonMode) {
        console.error(`   Transcript sauvegard√©: ${opts.transcript}`);
      }
    }
  } catch (e: any) {
    console.error(`\n‚ùå Erreur : ${e.message}`);
    process.exit(1);
  }
}

main();
